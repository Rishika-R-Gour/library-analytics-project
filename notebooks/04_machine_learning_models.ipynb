{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a83ff8",
   "metadata": {},
   "source": [
    "# ðŸ¤– Machine Learning & Predictive Analytics\n",
    "\n",
    "## ðŸŽ¯ **Advanced ML Models for Library Intelligence**\n",
    "\n",
    "Building sophisticated predictive models using our enhanced analytics database to drive actionable business insights.\n",
    "\n",
    "### ðŸ§  **ML Model Portfolio**\n",
    "\n",
    "#### **ðŸ“ˆ Demand Forecasting Models**\n",
    "- **Book Popularity Prediction**: Which books will be most requested next month?\n",
    "- **Seasonal Demand Patterns**: Inventory optimization for peak periods\n",
    "- **Branch-Level Forecasting**: Tailored predictions for each library location\n",
    "\n",
    "#### **ðŸ‘¤ Member Behavior Models**\n",
    "- **Churn Prediction**: Identify members at risk of leaving (90%+ accuracy target)\n",
    "- **Late Return Prediction**: Proactive intervention for overdue items\n",
    "- **Engagement Scoring**: Member lifetime value and engagement potential\n",
    "\n",
    "#### **ðŸŽ¯ Recommendation Systems**\n",
    "- **Collaborative Filtering**: \"Members like you also enjoyed...\"\n",
    "- **Content-Based Filtering**: Genre and author similarity matching\n",
    "- **Hybrid Recommendations**: Combined approach for 25%+ improvement\n",
    "\n",
    "#### **ðŸ“Š Operational Models**\n",
    "- **Staff Scheduling Optimization**: Predict busy periods for optimal staffing\n",
    "- **Collection Optimization**: Which books to acquire/retire based on data\n",
    "- **Revenue Prediction**: Membership and penalty revenue forecasting\n",
    "\n",
    "### ðŸŽ–ï¸ **Expected Business Impact**\n",
    "- **25%+ improvement** in recommendation relevance\n",
    "- **40%+ better inventory** management and demand prediction\n",
    "- **30%+ reduction** in member churn through early intervention\n",
    "- **20%+ efficiency gains** in operational resource allocation\n",
    "\n",
    "---\n",
    "*Leveraging our world-class enhanced analytics database for predictive excellence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f622b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Environment Setup - Simplified\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ¤– **MACHINE LEARNING ENVIRONMENT READY**\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸŽ¯ Core libraries loaded successfully\")\n",
    "print(\"ðŸ“Š Visualization libraries configured\")\n",
    "\n",
    "# Connect to our enhanced database\n",
    "try:\n",
    "    conn = sqlite3.connect('library.db')\n",
    "    print(\"ðŸ”Œ Connected to enhanced analytics database\")\n",
    "    \n",
    "    # Verify enhanced tables exist\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    tables = pd.read_sql_query(tables_query, conn)\n",
    "    \n",
    "    enhanced_tables = ['Publisher', 'Member_Preferences', 'Item_Reservations', 'Item_Reviews', 'Daily_Operations_Summary']\n",
    "    available_enhanced = [t for t in enhanced_tables if t in tables['name'].values]\n",
    "    \n",
    "    print(f\"âœ… Enhanced analytics tables available: {len(available_enhanced)}/5\")\n",
    "    print(f\"ðŸš€ Ready for ML modeling with enhanced data!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Database connection error: {e}\")\n",
    "    print(\"\udca1 Please ensure library.db exists in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e7312",
   "metadata": {},
   "source": [
    "## ðŸ“Š **Phase 1: Data Preparation & Feature Engineering**\n",
    "\n",
    "### Building the foundation for world-class predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Loading & Feature Engineering\n",
    "print(\"ðŸ“Š **LOADING ENHANCED DATASET FOR ML**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load core data with advanced joins\n",
    "ml_dataset_query = \"\"\"\n",
    "SELECT \n",
    "    -- Member Information\n",
    "    m.Member_ID,\n",
    "    m.Member_Type,\n",
    "    m.Join_Date,\n",
    "    julianday('now') - julianday(m.Join_Date) as Days_Since_Joining,\n",
    "    \n",
    "    -- Member Preferences (from enhanced table)\n",
    "    COALESCE(mp.Reading_Level, 'Unknown') as Reading_Level,\n",
    "    COALESCE(mp.Preferred_Format, 'Physical') as Preferred_Format,\n",
    "    COALESCE(mp.Email_Notifications, 0) as Email_Opt_In,\n",
    "    COALESCE(mp.Privacy_Level, 'Standard') as Privacy_Level,\n",
    "    \n",
    "    -- Loan History Aggregations\n",
    "    COUNT(DISTINCT l.Loan_ID) as Total_Loans,\n",
    "    COUNT(DISTINCT l.Item_ID) as Unique_Items_Borrowed,\n",
    "    AVG(julianday(l.Return_Date) - julianday(l.Loan_Date)) as Avg_Loan_Duration,\n",
    "    SUM(CASE WHEN l.Return_Date > l.Due_Date THEN 1 ELSE 0 END) as Late_Returns,\n",
    "    SUM(CASE WHEN l.Return_Date IS NULL AND l.Due_Date < date('now') THEN 1 ELSE 0 END) as Current_Overdue,\n",
    "    \n",
    "    -- Financial Metrics\n",
    "    COALESCE(SUM(l.Penalty_Amount), 0) as Total_Penalties,\n",
    "    \n",
    "    -- Recent Activity (last 90 days)\n",
    "    COUNT(CASE WHEN l.Loan_Date >= date('now', '-90 days') THEN 1 END) as Recent_Loans,\n",
    "    MAX(l.Loan_Date) as Last_Loan_Date,\n",
    "    julianday('now') - julianday(MAX(l.Loan_Date)) as Days_Since_Last_Loan,\n",
    "    \n",
    "    -- Engagement Metrics\n",
    "    COUNT(DISTINCT ir.Review_ID) as Total_Reviews,\n",
    "    AVG(CAST(ir.Rating as FLOAT)) as Avg_Rating_Given,\n",
    "    COUNT(DISTINCT res.Reservation_ID) as Total_Reservations\n",
    "    \n",
    "FROM Member m\n",
    "LEFT JOIN Member_Preferences mp ON m.Member_ID = mp.Member_ID\n",
    "LEFT JOIN Loan l ON m.Member_ID = l.Member_ID\n",
    "LEFT JOIN Item_Reviews ir ON m.Member_ID = ir.Member_ID\n",
    "LEFT JOIN Item_Reservations res ON m.Member_ID = res.Member_ID\n",
    "GROUP BY m.Member_ID, m.Member_Type, m.Join_Date, mp.Reading_Level, \n",
    "         mp.Preferred_Format, mp.Email_Opt_In, mp.Privacy_Level\n",
    "\"\"\"\n",
    "\n",
    "# Load the enhanced dataset\n",
    "ml_data = pd.read_sql_query(ml_dataset_query, conn)\n",
    "\n",
    "print(f\"âœ… Loaded {len(ml_data)} member records with enhanced features\")\n",
    "print(f\"ðŸ“Š Dataset shape: {ml_data.shape}\")\n",
    "print(f\"ðŸ” Features available: {ml_data.columns.tolist()}\")\n",
    "\n",
    "# Handle missing values intelligently\n",
    "ml_data['Days_Since_Last_Loan'] = ml_data['Days_Since_Last_Loan'].fillna(9999)  # Never borrowed\n",
    "ml_data['Avg_Loan_Duration'] = ml_data['Avg_Loan_Duration'].fillna(0)\n",
    "ml_data['Avg_Rating_Given'] = ml_data['Avg_Rating_Given'].fillna(0)\n",
    "\n",
    "# Create advanced engineered features\n",
    "print(\"\\nðŸ”§ **ADVANCED FEATURE ENGINEERING**\")\n",
    "\n",
    "# Engagement Score (composite metric)\n",
    "ml_data['Engagement_Score'] = (\n",
    "    ml_data['Total_Loans'] * 0.3 +\n",
    "    ml_data['Total_Reviews'] * 0.2 +\n",
    "    ml_data['Total_Reservations'] * 0.1 +\n",
    "    (ml_data['Email_Opt_In'] * 10) * 0.1 +\n",
    "    (10 - np.minimum(ml_data['Days_Since_Last_Loan'] / 10, 10)) * 0.3\n",
    ")\n",
    "\n",
    "# Risk Indicators\n",
    "ml_data['Late_Return_Rate'] = ml_data['Late_Returns'] / np.maximum(ml_data['Total_Loans'], 1)\n",
    "ml_data['High_Risk'] = (\n",
    "    (ml_data['Late_Return_Rate'] > 0.3) | \n",
    "    (ml_data['Days_Since_Last_Loan'] > 180) |\n",
    "    (ml_data['Total_Penalties'] > 50)\n",
    ").astype(int)\n",
    "\n",
    "# Member Lifecycle Stage\n",
    "def member_lifecycle(row):\n",
    "    if row['Days_Since_Joining'] < 30:\n",
    "        return 'New'\n",
    "    elif row['Days_Since_Last_Loan'] > 365:\n",
    "        return 'Dormant'\n",
    "    elif row['Days_Since_Last_Loan'] > 90:\n",
    "        return 'At_Risk'\n",
    "    elif row['Total_Loans'] > 20 and row['Days_Since_Last_Loan'] < 30:\n",
    "        return 'Power_User'\n",
    "    else:\n",
    "        return 'Active'\n",
    "\n",
    "ml_data['Lifecycle_Stage'] = ml_data.apply(member_lifecycle, axis=1)\n",
    "\n",
    "# Value Segmentation\n",
    "loan_quartiles = ml_data['Total_Loans'].quantile([0.25, 0.5, 0.75])\n",
    "def value_segment(loans):\n",
    "    if loans <= loan_quartiles[0.25]:\n",
    "        return 'Low_Value'\n",
    "    elif loans <= loan_quartiles[0.5]:\n",
    "        return 'Medium_Value'\n",
    "    elif loans <= loan_quartiles[0.75]:\n",
    "        return 'High_Value'\n",
    "    else:\n",
    "        return 'Premium_Value'\n",
    "\n",
    "ml_data['Value_Segment'] = ml_data['Total_Loans'].apply(value_segment)\n",
    "\n",
    "print(f\"âœ… Advanced features engineered:\")\n",
    "print(f\"   ðŸ“Š Engagement Score (0-100 scale)\")\n",
    "print(f\"   âš ï¸  High Risk Indicator (binary)\")\n",
    "print(f\"   ðŸ”„ Member Lifecycle Stage (5 categories)\")\n",
    "print(f\"   ðŸ’Ž Value Segmentation (4 tiers)\")\n",
    "\n",
    "# Display feature engineering results\n",
    "print(f\"\\nðŸ“ˆ **FEATURE ENGINEERING SUMMARY**:\")\n",
    "print(f\"   Lifecycle Stages: {ml_data['Lifecycle_Stage'].value_counts().to_dict()}\")\n",
    "print(f\"   Value Segments: {ml_data['Value_Segment'].value_counts().to_dict()}\")\n",
    "print(f\"   High Risk Members: {ml_data['High_Risk'].sum()}/{len(ml_data)} ({ml_data['High_Risk'].mean():.1%})\")\n",
    "print(f\"   Avg Engagement Score: {ml_data['Engagement_Score'].mean():.1f}\")\n",
    "\n",
    "# Preview the enhanced dataset\n",
    "print(f\"\\nðŸ“‹ **ENHANCED ML DATASET PREVIEW**:\")\n",
    "display_cols = ['Member_ID', 'Member_Type', 'Total_Loans', 'Engagement_Score', \n",
    "                'Lifecycle_Stage', 'Value_Segment', 'High_Risk']\n",
    "print(ml_data[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aef0ae",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ **Phase 2: Member Churn Prediction Model**\n",
    "\n",
    "### Building a high-accuracy model to identify members at risk of leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1644e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Member Churn Prediction Model\n",
    "print(\"ðŸŽ¯ **MEMBER CHURN PREDICTION MODEL**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define churn based on multiple criteria\n",
    "def define_churn(row):\n",
    "    # Churn if no activity in 6+ months OR very high penalty amounts\n",
    "    return (\n",
    "        (row['Days_Since_Last_Loan'] > 180) |\n",
    "        (row['Total_Penalties'] > 100) |\n",
    "        (row['Late_Return_Rate'] > 0.5 and row['Total_Loans'] > 5)\n",
    "    )\n",
    "\n",
    "ml_data['Churn_Risk'] = ml_data.apply(define_churn, axis=1).astype(int)\n",
    "\n",
    "print(f\"ðŸ“Š Churn Distribution:\")\n",
    "churn_dist = ml_data['Churn_Risk'].value_counts()\n",
    "print(f\"   Active Members: {churn_dist.get(0, 0)} ({churn_dist.get(0, 0)/len(ml_data):.1%})\")\n",
    "print(f\"   At-Risk Members: {churn_dist.get(1, 0)} ({churn_dist.get(1, 0)/len(ml_data):.1%})\")\n",
    "\n",
    "# Prepare features for churn prediction\n",
    "churn_features = [\n",
    "    'Days_Since_Joining', 'Total_Loans', 'Unique_Items_Borrowed',\n",
    "    'Avg_Loan_Duration', 'Late_Return_Rate', 'Total_Penalties',\n",
    "    'Recent_Loans', 'Days_Since_Last_Loan', 'Total_Reviews',\n",
    "    'Total_Reservations', 'Engagement_Score', 'Email_Opt_In'\n",
    "]\n",
    "\n",
    "# Handle categorical variables\n",
    "le_member_type = LabelEncoder()\n",
    "le_reading_level = LabelEncoder()\n",
    "le_format = LabelEncoder()\n",
    "le_privacy = LabelEncoder()\n",
    "\n",
    "ml_data['Member_Type_Encoded'] = le_member_type.fit_transform(ml_data['Member_Type'])\n",
    "ml_data['Reading_Level_Encoded'] = le_reading_level.fit_transform(ml_data['Reading_Level'])\n",
    "ml_data['Format_Encoded'] = le_format.fit_transform(ml_data['Preferred_Format'])\n",
    "ml_data['Privacy_Encoded'] = le_privacy.fit_transform(ml_data['Privacy_Level'])\n",
    "\n",
    "churn_features.extend(['Member_Type_Encoded', 'Reading_Level_Encoded', 'Format_Encoded', 'Privacy_Encoded'])\n",
    "\n",
    "# Prepare training data\n",
    "X_churn = ml_data[churn_features]\n",
    "y_churn = ml_data['Churn_Risk']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ **CHURN PREDICTION TRAINING**:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Features: {len(churn_features)}\")\n",
    "\n",
    "# Train multiple models\n",
    "churn_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "churn_results = {}\n",
    "\n",
    "for name, model in churn_models.items():\n",
    "    # Train model\n",
    "    if name == 'SVM':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    churn_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {name} Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Precision: {precision:.3f}\")\n",
    "    print(f\"   Recall: {recall:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Select best model\n",
    "best_churn_model = max(churn_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"\\nðŸ† **BEST CHURN MODEL**: {best_churn_model[0]} (F1: {best_churn_model[1]['f1']:.3f})\")\n",
    "\n",
    "# Feature importance for best model\n",
    "if best_churn_model[0] in ['Random Forest', 'Gradient Boosting']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': churn_features,\n",
    "        'importance': best_churn_model[1]['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nðŸ” **TOP 10 CHURN PREDICTION FEATURES**:\")\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f06bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn Prediction Visualization\n",
    "print(\"ðŸ“Š **CHURN PREDICTION VISUALIZATION**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create visualization dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax1 = axes[0, 0]\n",
    "model_names = list(churn_results.keys())\n",
    "f1_scores = [results['f1'] for results in churn_results.values()]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "bars = ax1.bar(model_names, f1_scores, color=colors)\n",
    "ax1.set_title('ðŸ† Churn Prediction Model Comparison\\n(F1-Score)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Feature Importance (if available)\n",
    "ax2 = axes[0, 1]\n",
    "if best_churn_model[0] in ['Random Forest', 'Gradient Boosting']:\n",
    "    top_features = feature_importance.head(8)\n",
    "    bars = ax2.barh(top_features['feature'], top_features['importance'], color='#FF6B6B')\n",
    "    ax2.set_title('ðŸ” Top Churn Prediction Features', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Feature Importance')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Feature Importance\\nNot Available\\nfor Selected Model',\n",
    "             ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('ðŸ” Feature Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Churn Risk Distribution\n",
    "ax3 = axes[1, 0]\n",
    "churn_counts = ml_data['Churn_Risk'].value_counts()\n",
    "labels = ['Active Members', 'At-Risk Members']\n",
    "sizes = [churn_counts.get(0, 0), churn_counts.get(1, 0)]\n",
    "colors_pie = ['#4ECDC4', '#FF6B6B']\n",
    "explode = (0, 0.1)  # explode the at-risk slice\n",
    "\n",
    "ax3.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%',\n",
    "        startangle=90, explode=explode)\n",
    "ax3.set_title('âš ï¸ Member Churn Risk Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Engagement Score vs Churn Risk\n",
    "ax4 = axes[1, 1]\n",
    "churn_0 = ml_data[ml_data['Churn_Risk'] == 0]['Engagement_Score']\n",
    "churn_1 = ml_data[ml_data['Churn_Risk'] == 1]['Engagement_Score']\n",
    "\n",
    "ax4.hist([churn_0, churn_1], bins=20, alpha=0.7, \n",
    "         label=['Active', 'At-Risk'], color=['#4ECDC4', '#FF6B6B'])\n",
    "ax4.set_title('ðŸ“Š Engagement Score Distribution\\nby Churn Risk', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Engagement Score')\n",
    "ax4.set_ylabel('Number of Members')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Actionable Insights\n",
    "print(f\"\\nðŸŽ¯ **ACTIONABLE CHURN INSIGHTS**:\")\n",
    "at_risk_members = ml_data[ml_data['Churn_Risk'] == 1]\n",
    "print(f\"   ðŸ“ˆ {len(at_risk_members)} members identified as at-risk\")\n",
    "print(f\"   ðŸ’¡ Average days since last loan for at-risk: {at_risk_members['Days_Since_Last_Loan'].mean():.0f}\")\n",
    "print(f\"   ðŸ“§ Email opt-in rate for at-risk: {at_risk_members['Email_Opt_In'].mean():.1%}\")\n",
    "print(f\"   ðŸ’° Average penalties for at-risk: ${at_risk_members['Total_Penalties'].mean():.2f}\")\n",
    "\n",
    "# Identify top at-risk members for intervention\n",
    "best_model_proba = best_churn_model[1]['probabilities']\n",
    "X_test_with_proba = X_test.copy()\n",
    "X_test_with_proba['Churn_Probability'] = best_model_proba\n",
    "top_at_risk = X_test_with_proba.nlargest(10, 'Churn_Probability')\n",
    "\n",
    "print(f\"\\nðŸš¨ **TOP 10 MEMBERS FOR IMMEDIATE INTERVENTION**:\")\n",
    "print(f\"   (Based on highest churn probability)\")\n",
    "for idx, (_, row) in enumerate(top_at_risk.iterrows(), 1):\n",
    "    member_id = ml_data.iloc[row.name]['Member_ID']\n",
    "    prob = row['Churn_Probability']\n",
    "    print(f\"   {idx}. Member {member_id}: {prob:.1%} churn risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec870876",
   "metadata": {},
   "source": [
    "## ðŸ“š **Phase 3: Book Demand Forecasting Model**\n",
    "\n",
    "### Predicting which books will be most popular for optimal inventory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book Demand Forecasting Model\n",
    "print(\"ðŸ“š **BOOK DEMAND FORECASTING MODEL**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load book popularity data\n",
    "book_demand_query = \"\"\"\n",
    "SELECT \n",
    "    i.Item_ID,\n",
    "    i.Title,\n",
    "    i.Genre,\n",
    "    i.Publication_Year,\n",
    "    2025 - i.Publication_Year as Book_Age,\n",
    "    \n",
    "    -- Loan metrics\n",
    "    COUNT(DISTINCT l.Loan_ID) as Total_Loans,\n",
    "    COUNT(DISTINCT l.Member_ID) as Unique_Borrowers,\n",
    "    AVG(julianday(l.Return_Date) - julianday(l.Loan_Date)) as Avg_Loan_Duration,\n",
    "    \n",
    "    -- Recent popularity (last 90 days)\n",
    "    COUNT(CASE WHEN l.Loan_Date >= date('now', '-90 days') THEN 1 END) as Recent_Loans,\n",
    "    \n",
    "    -- Review metrics\n",
    "    COUNT(DISTINCT ir.Review_ID) as Total_Reviews,\n",
    "    AVG(CAST(ir.Rating as FLOAT)) as Avg_Rating,\n",
    "    COUNT(CASE WHEN ir.Would_Recommend = 1 THEN 1 END) as Recommendations,\n",
    "    \n",
    "    -- Reservation metrics\n",
    "    COUNT(DISTINCT res.Reservation_ID) as Total_Reservations,\n",
    "    COUNT(CASE WHEN res.Status = 'Active' THEN 1 END) as Active_Reservations\n",
    "    \n",
    "FROM Item i\n",
    "LEFT JOIN Loan l ON i.Item_ID = l.Item_ID\n",
    "LEFT JOIN Item_Reviews ir ON i.Item_ID = ir.Item_ID\n",
    "LEFT JOIN Item_Reservations res ON i.Item_ID = res.Item_ID\n",
    "GROUP BY i.Item_ID, i.Title, i.Genre, i.Publication_Year\n",
    "HAVING COUNT(DISTINCT l.Loan_ID) > 0  -- Only books that have been borrowed\n",
    "\"\"\"\n",
    "\n",
    "book_data = pd.read_sql_query(book_demand_query, conn)\n",
    "\n",
    "print(f\"âœ… Loaded {len(book_data)} books with demand history\")\n",
    "\n",
    "# Handle missing values\n",
    "book_data['Avg_Rating'] = book_data['Avg_Rating'].fillna(book_data['Avg_Rating'].mean())\n",
    "book_data['Avg_Loan_Duration'] = book_data['Avg_Loan_Duration'].fillna(book_data['Avg_Loan_Duration'].mean())\n",
    "\n",
    "# Create demand categories (target variable)\n",
    "# High demand: top 25%, Medium: middle 50%, Low: bottom 25%\n",
    "demand_thresholds = book_data['Total_Loans'].quantile([0.25, 0.75])\n",
    "\n",
    "def demand_category(loans):\n",
    "    if loans >= demand_thresholds[0.75]:\n",
    "        return 'High'\n",
    "    elif loans >= demand_thresholds[0.25]:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "book_data['Demand_Category'] = book_data['Total_Loans'].apply(demand_category)\n",
    "\n",
    "print(f\"\\nðŸ“Š Demand Distribution:\")\n",
    "demand_dist = book_data['Demand_Category'].value_counts()\n",
    "for category, count in demand_dist.items():\n",
    "    print(f\"   {category} Demand: {count} books ({count/len(book_data):.1%})\")\n",
    "\n",
    "# Engineer features for demand prediction\n",
    "# Genre popularity impact\n",
    "genre_popularity = book_data.groupby('Genre')['Total_Loans'].mean().to_dict()\n",
    "book_data['Genre_Popularity'] = book_data['Genre'].map(genre_popularity)\n",
    "\n",
    "# Recency factor (how recent the book is)\n",
    "book_data['Recency_Score'] = np.maximum(0, 10 - (book_data['Book_Age'] / 5))  # Newer books score higher\n",
    "\n",
    "# Engagement ratio\n",
    "book_data['Review_Engagement'] = book_data['Total_Reviews'] / np.maximum(book_data['Total_Loans'], 1)\n",
    "book_data['Reservation_Ratio'] = book_data['Total_Reservations'] / np.maximum(book_data['Total_Loans'], 1)\n",
    "\n",
    "# Momentum indicator (recent vs historical performance)\n",
    "book_data['Momentum'] = book_data['Recent_Loans'] / np.maximum(book_data['Total_Loans'] / 4, 1)  # Quarterly comparison\n",
    "\n",
    "# Prepare features for demand prediction\n",
    "demand_features = [\n",
    "    'Book_Age', 'Unique_Borrowers', 'Avg_Loan_Duration', 'Avg_Rating',\n",
    "    'Total_Reviews', 'Recommendations', 'Genre_Popularity', 'Recency_Score',\n",
    "    'Review_Engagement', 'Reservation_Ratio', 'Momentum', 'Active_Reservations'\n",
    "]\n",
    "\n",
    "# Encode genre\n",
    "le_genre = LabelEncoder()\n",
    "book_data['Genre_Encoded'] = le_genre.fit_transform(book_data['Genre'])\n",
    "demand_features.append('Genre_Encoded')\n",
    "\n",
    "# Prepare training data\n",
    "X_demand = book_data[demand_features]\n",
    "y_demand = book_data['Demand_Category']\n",
    "\n",
    "# Encode target variable\n",
    "le_demand = LabelEncoder()\n",
    "y_demand_encoded = le_demand.fit_transform(y_demand)\n",
    "\n",
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_demand, y_demand_encoded, test_size=0.2, random_state=42, stratify=y_demand_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“š **DEMAND FORECASTING TRAINING**:\")\n",
    "print(f\"   Training samples: {len(X_train_d)}\")\n",
    "print(f\"   Test samples: {len(X_test_d)}\")\n",
    "print(f\"   Features: {len(demand_features)}\")\n",
    "\n",
    "# Train demand forecasting models\n",
    "demand_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "demand_results = {}\n",
    "\n",
    "for name, model in demand_models.items():\n",
    "    model.fit(X_train_d, y_train_d)\n",
    "    y_pred_d = model.predict(X_test_d)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_d, y_pred_d)\n",
    "    f1 = f1_score(y_test_d, y_pred_d, average='weighted')\n",
    "    \n",
    "    demand_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred_d\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {name} Demand Prediction:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Select best demand model\n",
    "best_demand_model = max(demand_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"\\nðŸ† **BEST DEMAND MODEL**: {best_demand_model[0]} (F1: {best_demand_model[1]['f1']:.3f})\")\n",
    "\n",
    "# Feature importance for demand prediction\n",
    "if best_demand_model[0] in ['Random Forest', 'Gradient Boosting']:\n",
    "    demand_importance = pd.DataFrame({\n",
    "        'feature': demand_features,\n",
    "        'importance': best_demand_model[1]['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nðŸ” **TOP DEMAND PREDICTION FEATURES**:\")\n",
    "    for idx, row in demand_importance.head(8).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# Predict future high-demand books\n",
    "all_predictions = best_demand_model[1]['model'].predict(X_demand)\n",
    "all_probabilities = best_demand_model[1]['model'].predict_proba(X_demand)\n",
    "\n",
    "# Get high-demand probability (assuming class 1 is High)\n",
    "high_demand_class_idx = list(le_demand.classes_).index('High')\n",
    "book_data['High_Demand_Probability'] = all_probabilities[:, high_demand_class_idx]\n",
    "book_data['Predicted_Demand'] = le_demand.inverse_transform(all_predictions)\n",
    "\n",
    "# Identify books to prioritize for acquisition/promotion\n",
    "high_potential_books = book_data[\n",
    "    (book_data['High_Demand_Probability'] > 0.7) & \n",
    "    (book_data['Recent_Loans'] < 5)  # Currently underperforming\n",
    "].sort_values('High_Demand_Probability', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ **HIGH-POTENTIAL BOOKS FOR PROMOTION**:\")\n",
    "for idx, (_, book) in enumerate(high_potential_books.head(10).iterrows(), 1):\n",
    "    print(f\"   {idx}. {book['Title'][:40]}... | {book['Genre']} | {book['High_Demand_Probability']:.1%} potential\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ **DEMAND FORECASTING INSIGHTS**:\")\n",
    "print(f\"   ðŸŽ¯ {len(high_potential_books)} books identified as high-potential\")\n",
    "print(f\"   ðŸ“š Average age of high-potential books: {high_potential_books['Book_Age'].mean():.1f} years\")\n",
    "print(f\"   â­ Average rating of high-potential books: {high_potential_books['Avg_Rating'].mean():.1f}/5.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
